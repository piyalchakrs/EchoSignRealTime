import os
import numpy as np
import pandas as pd

from typing import Any, Dict, cast

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
SRC_DIR = os.path.join(os.path.dirname(__file__), "..", "src")

CSV_PATH = os.path.join(DATA_DIR, "dataset.csv")

# Strict, global feature order â€“ MUST match ESP32:
# f1,f2,f3,f4,f5,gdp,ax,ay,az,gx,gy,gz
FEATURE_COLS = [
    "f1", "f2", "f3", "f4", "f5",
    "gdp",
    "ax", "ay", "az",
    "gx", "gy", "gz",
]

LABEL_COL = "label"


def export_scaler_params(scaler: StandardScaler, out_path: str) -> None:
    # Force numpy arrays with dtype float so type-checkers are happy
    mean = np.asarray(scaler.mean_, dtype=float)
    scale = np.asarray(scaler.scale_, dtype=float)
    num_features = int(mean.shape[0])

    lines = []
    lines.append("#pragma once")
    lines.append("#include <Arduino.h>")
    lines.append("")
    lines.append(f"#define NUM_FEATURES {num_features}")
    lines.append("")
    mean_vals = ", ".join(f"{float(m):.6f}f" for m in mean)
    lines.append(f"static const float SCALER_MEAN[NUM_FEATURES] = {{ {mean_vals} }};")
    scale_vals = ", ".join(f"{float(s):.6f}f" for s in scale)
    lines.append(f"static const float SCALER_SCALE[NUM_FEATURES] = {{ {scale_vals} }};")
    lines.append("")
    lines.append("inline void standardizeFeatures(float feat[NUM_FEATURES]) {")
    lines.append("  for (int i = 0; i < NUM_FEATURES; ++i) {")
    lines.append("    feat[i] = (feat[i] - SCALER_MEAN[i]) / SCALER_SCALE[i];")
    lines.append("  }")
    lines.append("}")
    lines.append("")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Wrote scaler params to {out_path}")


def export_label_names(le: LabelEncoder, out_path: str) -> None:
    classes = list(le.classes_)
    num = len(classes)

    lines = []
    lines.append("#pragma once")
    lines.append("")
    lines.append(f"#define NUM_CLASSES {num}")
    lines.append("")
    lines.append("static const char* const label_names[NUM_CLASSES] = {")
    for i, name in enumerate(classes):
        safe = str(name).replace("\\", "\\\\").replace('"', '\\"')
        comma = "," if i < num - 1 else ""
        lines.append(f'  "{safe}"{comma}')
    lines.append("};")
    lines.append("")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Wrote label names to {out_path}")


def export_knn_model(
    knn: KNeighborsClassifier,
    scaler: StandardScaler,
    le: LabelEncoder,
    X_scaled: np.ndarray,
    y_enc: np.ndarray,
    out_path: str,
) -> None:
    n_samples, n_features = X_scaled.shape

    # metric / weights / n_neighbors via getattr so Pylance is happy
    metric = getattr(knn, "metric", "euclidean")
    if metric == "euclidean":
        metric_code = 0
    elif metric == "manhattan":
        metric_code = 1
    elif metric == "chebyshev":
        metric_code = 2
    else:
        raise ValueError(f"Unsupported metric for export: {metric}")

    weights = getattr(knn, "weights", "uniform")
    if weights == "uniform":
        weights_code = 0
    elif weights == "distance":
        weights_code = 1
    else:
        raise ValueError(f"Unsupported weights for export: {weights}")

    k = int(getattr(knn, "n_neighbors", 3))

    lines = []
    lines.append("#pragma once")
    lines.append("#include <Arduino.h>")
    lines.append("#include \"scaler_params.h\"")
    lines.append("")
    lines.append("// Auto-generated by train_knn.py")
    lines.append(f"#define KNN_K        {k}")
    lines.append(f"#define KNN_METRIC   {metric_code}  // 0=L2, 1=L1, 2=Linf")
    lines.append(f"#define KNN_WEIGHTS  {weights_code}  // 0=uniform, 1=distance")
    lines.append("")
    lines.append(f"#define NUM_SAMPLES  {n_samples}")
    lines.append("")
    lines.append("static const float X_train[NUM_SAMPLES][NUM_FEATURES] PROGMEM = {")
    for i in range(n_samples):
        row = ", ".join(f"{float(v):.6f}f" for v in X_scaled[i])
        comma = "," if i < n_samples - 1 else ""
        lines.append(f"  {{ {row} }}{comma}")
    lines.append("};")
    lines.append("")
    lines.append("static const uint8_t y_train[NUM_SAMPLES] PROGMEM = {")
    row = ", ".join(str(int(v)) for v in y_enc)
    lines.append(f"  {row}")
    lines.append("};")
    lines.append("")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Wrote KNN model to {out_path}")


def main() -> None:
    if not os.path.exists(CSV_PATH):
        raise SystemExit(f"dataset.csv not found at {CSV_PATH}")

    df = pd.read_csv(CSV_PATH)

    missing = [c for c in FEATURE_COLS + [LABEL_COL] if c not in df.columns]
    if missing:
        raise SystemExit(f"dataset.csv missing columns: {missing}")

    # Force numpy arrays so types are clean
    X: np.ndarray = df[FEATURE_COLS].to_numpy(dtype=float)
    y_str: np.ndarray = df[LABEL_COL].astype(str).to_numpy()

    le = LabelEncoder()
    y_enc: np.ndarray = np.asarray(le.fit_transform(y_str), dtype=int)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_enc, test_size=0.2, stratify=y_enc, random_state=42
    )

    # Compact hyperparameter sweep
    candidates = []
    for k in (5, 7, 9):
        for metric in ("manhattan", "chebyshev"):
            candidates.append({"n_neighbors": k, "metric": metric, "weights": "distance"})

    best_cfg = None
    best_macro = -1.0
    best_weighted = -1.0
    reports = []

    # Use plain Python strings for class names so dict lookups are well-typed
    class_names = [str(c) for c in le.classes_]

    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    for cfg in candidates:
        pipe = Pipeline([
            ("scaler", StandardScaler()),
            ("knn", KNeighborsClassifier(n_neighbors=cfg["n_neighbors"], metric=cfg["metric"], weights=cfg["weights"]))
        ])

        y_pred_cv = cross_val_predict(pipe, X, y_enc, cv=cv, n_jobs=-1)
        report: Dict[str, Any] = cast(Dict[str, Any], classification_report(
            y_enc, y_pred_cv, target_names=class_names, output_dict=True, zero_division=0
        ))
        macro_f1 = float(report.get("macro avg", {}).get("f1-score", 0.0))
        weighted_f1 = float(report.get("weighted avg", {}).get("f1-score", 0.0))

        # Store textual summary line for printing later
        per_class = {label: float(report[label]["f1-score"]) for label in class_names if label in report}
        reports.append((cfg, macro_f1, weighted_f1, per_class))

        # Track best configuration by weighted F1 (tie-breaker by macro F1)
        if (weighted_f1 > best_weighted) or (weighted_f1 == best_weighted and macro_f1 > best_macro):
            best_weighted = weighted_f1
            best_macro = macro_f1
            best_cfg = cfg

    print("\nSweep results (macro-F1 then per-class F1):")
    for cfg, macro_f1, weighted_f1, per_class in reports:
        pcs = ", ".join(f"{lbl}:{per_class.get(lbl, 0.0):.3f}" for lbl in class_names)
        print(f"  k={cfg['n_neighbors']}, metric={cfg['metric']}, weights=distance -> macro-F1={macro_f1:.4f}, weighted-F1={weighted_f1:.4f} | {pcs}")

    if best_cfg is None:
        raise SystemExit("No valid hyperparameter configuration evaluated.")

    print("\nSelected best config:", best_cfg, f"macro-F1={best_macro:.4f}")

    # Fit best on full dataset, then do a simple holdout for visibility
    best_pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("knn", KNeighborsClassifier(n_neighbors=best_cfg["n_neighbors"], metric=best_cfg["metric"], weights=best_cfg["weights"]))
    ])
    best_pipe.fit(X, y_enc)

    # Also show a quick train/test split report for sanity
    X_tr, X_te, y_tr, y_te = train_test_split(X, y_enc, test_size=0.2, stratify=y_enc, random_state=42)
    best_pipe.fit(X_tr, y_tr)
    y_pred = best_pipe.predict(X_te)
    print("\nHoldout classification report (20% test):")
    print(classification_report(y_te, y_pred, target_names=le.classes_))
    print("Confusion matrix:")
    print(confusion_matrix(y_te, y_pred))

    scaler: StandardScaler = best_pipe.named_steps["scaler"]  # type: ignore[assignment]
    knn: KNeighborsClassifier = best_pipe.named_steps["knn"]  # type: ignore[assignment]

    os.makedirs(SRC_DIR, exist_ok=True)

    export_scaler_params(scaler, os.path.join(SRC_DIR, "scaler_params.h"))
    export_label_names(le, os.path.join(SRC_DIR, "label_names.h"))

    # Fit on all data to get final scaler, then build an export set
    best_pipe.fit(X, y_enc)
    scaler = best_pipe.named_steps["scaler"]  # type: ignore[assignment]
    knn = best_pipe.named_steps["knn"]  # type: ignore[assignment]
    X_all_scaled: np.ndarray = np.asarray(scaler.transform(X), dtype=float)
    y_all_enc: np.ndarray = np.asarray(le.transform(y_str), dtype=np.uint8)

    # Reduce firmware size: cap samples per class for export
    # Default cap can be overridden via env MAX_SAMPLES_PER_CLASS
    try:
        max_per_class = int(os.getenv("MAX_SAMPLES_PER_CLASS", "800"))
    except Exception:
        max_per_class = 800

    rng = np.random.default_rng(42)
    kept_indices = []
    per_class_counts = []
    for c, _name in enumerate(le.classes_):
        cls_idx = np.where(y_all_enc == c)[0]
        if cls_idx.size > max_per_class:
            sel = rng.choice(cls_idx, size=max_per_class, replace=False)
        else:
            sel = cls_idx
        kept_indices.append(sel)
        per_class_counts.append((int(c), _name, int(sel.size)))

    kept_indices = np.concatenate(kept_indices)
    rng.shuffle(kept_indices)

    X_export = X_all_scaled[kept_indices]
    y_export = y_all_enc[kept_indices]

    total_kept = int(X_export.shape[0])
    approx_bytes = total_kept * X_export.shape[1] * 4 + total_kept  # floats + labels
    print(f"\nExport capping per class (max {max_per_class}). kept={total_kept} samples, ~{approx_bytes/1024:.1f} KB for X/y")
    print("Per-class kept counts:")
    for c, name, cnt in per_class_counts:
        print(f"  {name}: {cnt}")

    export_knn_model(
        knn,
        scaler,
        le,
        X_export,
        y_export,
        os.path.join(SRC_DIR, "glove_knn_model.h"),
    )

    print("\nAll headers exported. Rebuild the firmware in PlatformIO.")

if __name__ == "__main__":
    main()
